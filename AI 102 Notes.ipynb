{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applied AI Services\n",
    "You can use Cognitive Services to build your own AI solutions, and they also underpin Azure Applied AI Services that provide out-of-the-box solutions for common AI scenarios. Applied AI Services include:\n",
    "\n",
    "1. Azure Form Recognizer - an optical character recognition (OCR) solution that can extract semantic meaning from forms, such as invoices, receipts, and others.<br>\n",
    "2. Azure Metrics Advisor - A service build on the Anomaly Detector cognitive service that simplifies real-time monitoring and response to critical metrics.<br>\n",
    "3. Azure Video Analyzer for Media - A comprehensive video analysts solution build on the Video Indexer cognitive service.<br>\n",
    "4. Azure Immersive Reader - A reading solution that supports people of all ages and abilities.<br>\n",
    "5. Azure Bot Service - A cloud service for delivering conversational AI solutions, or bots.<br>\n",
    "6. Azure Cognitive Search - A cloud-scale search solution that uses cognitive services to extract insights from data and documents.\n",
    "\n",
    "AI engineers can develop Bots by writing code, using the classes available in the Bot Framework SDK. Alternatively, you can use the Bot Framework Composer to develop complex bots using a visual design interface.\n",
    "\n",
    "Azure Cognitive Search is an Applied AI Service that enables you to ingest and index data from various sources, and search the index to find, filter, and sort information extracted from the source data.\n",
    "\n",
    "In addition to basic text-based indexing, Azure Cognitive Search enables you to define an enrichment pipeline that uses AI skills to enhance the index with insights derived from the source data - for example, by using computer vision and natural language processing capabilities to generate descriptions of images, extract text from scanned documents, and determine key phrases in large documents that encapsulate their key points.\n",
    "\n",
    "Not only does this AI enrichment produce a more useful search experience, the insights extracted by your enrichment pipeline can be persisted in a knowledge store for further analysis or integration into a data pipeline for a business intelligence solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options for Azure resources\n",
    "For many of the available cognitive services, you can choose between the following provisioning options:\n",
    "\n",
    "Multi-service resource\n",
    "You can provision a Cognitive Services resource that supports multiple different cognitive services. For example, you could create a single resource that enables you to use the Language, Computer Vision, Speech, and other services.\n",
    "\n",
    "This approach enables you to manage a single set of access credentials to consume multiple services at a single endpoint, and with a single point of billing for usage of all services.\n",
    "\n",
    "Single-service resource\n",
    "Each cognitive service can be provisioned individually, for example by creating discrete Language and Computer Vision resources in your Azure subscription.\n",
    "\n",
    "This approach enables you to use separate endpoints for each service (for example to provision them in different geographical regions) and to manage access credentials for each service independently. It also enables you to manage billing separately for each service.\n",
    "\n",
    "Single-service resources generally offer a free tier (with usage restrictions), making them a good choice to try out a service before using it in a production application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Provisioning cognitive services</h3>\n",
    "\n",
    "Start Visual Studio Code.\n",
    "\n",
    "Open the palette (SHIFT+CTRL+P) and run a Git: Clone command to clone the https://github.com/MicrosoftLearning/AI-102-AIEngineer repository to a local folder (it doesn't matter which folder).\n",
    "\n",
    "When the repository has been cloned, open the folder in Visual Studio Code.\n",
    "\n",
    "Wait while additional files are installed to support the C# code projects in the repo.\n",
    "\n",
    "<h3>Steps:</h3>\n",
    "Open the Azure portal at https://portal.azure.com, and sign in using the Microsoft account associated with your Azure subscription.\n",
    "Select the ＋Create a resource button, search for cognitive services, and create a Cognitive Services resource with the following settings:\n",
    "Subscription: Your Azure subscription\n",
    "Resource group: Choose or create a resource group (if you are using a restricted subscription, you may not have permission to create a new resource group - use the one provided)\n",
    "Region: Choose any available region\n",
    "Name: Enter a unique name\n",
    "Pricing tier: Standard S0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Use a REST Interface\n",
    "The cognitive services APIs are REST-based, so you can consume them by submitting JSON requests over HTTP. In this example, you'll explore a console application that uses the Language REST API to perform language detection; but the basic principle is the same for all of the APIs supported by the Cognitive Services resource.\n",
    "\n",
    "Note: In this exercise, you can choose to use the REST API from either C# or Python. In the steps below, perform the actions appropriate for your preferred language.\n",
    "\n",
    "In Visual Studio Code, in the Explorer pane, browse to the 01-getting-started folder and expand the C-Sharp or Python folder depending on your language preference.\n",
    "\n",
    "View the contents of the rest-client folder, and note that it contains a file for configuration settings:\n",
    "\n",
    "C#: appsettings.json\n",
    "Python: .env\n",
    "Open the configuration file and update the configuration values it contains to reflect the endpoint and an authentication key for your cognitive services resource. Save your changes.\n",
    "\n",
    "Note that the rest-client folder contains a code file for the client application:\n",
    "\n",
    "C#: Program.cs\n",
    "Python: rest-client.py\n",
    "Open the code file and review the code it contains, noting the following details:\n",
    "\n",
    "Various namespaces are imported to enable HTTP communication\n",
    "Code in the Main function retrieves the endpoint and key for your cognitive services resource - these will be used to send REST requests to the Text Analytics service.\n",
    "The program accepts user input, and uses the GetLanguage function to call the Text Analytics language detection REST API for your cognitive services endpoint to detect the language of the text that was entered.\n",
    "The request sent to the API consists of a JSON object containing the input data - in this case, a collection of document objects, each of which has an id and text.\n",
    "The key for your service is included in the request header to authenticate your client application.\n",
    "The response from the service is a JSON object, which the client application can parse.\n",
    "Right-click the rest-client folder and open an integrated terminal. Then enter the following language-specific command to run the program:\n",
    "\n",
    "C#\n",
    "\n",
    "dotnet run\n",
    "Python\n",
    "\n",
    "python rest-client.py\n",
    "When prompted, enter some text and review the language that is detected by the service, which is returned in the JSON response. For example, try entering \"Hello\", \"Bonjour\", and \"Hola\".\n",
    "\n",
    "When you have finished testing the application, enter \"quit\" to stop the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-469b56f740ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhttp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0murllib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "#rest-client.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import http.client, base64, json, urllib\n",
    "from urllib import request, parse, error\n",
    "\n",
    "def main():\n",
    "    global cog_endpoint\n",
    "    global cog_key\n",
    "\n",
    "    try:\n",
    "        # Get Configuration Settings\n",
    "        load_dotenv()\n",
    "        cog_endpoint = os.getenv('COG_SERVICE_ENDPOINT')\n",
    "        cog_key = os.getenv('COG_SERVICE_KEY')\n",
    "\n",
    "        # Get user input (until they enter \"quit\")\n",
    "        userText =''\n",
    "        while userText.lower() != 'quit':\n",
    "            userText = input('Enter some text (\"quit\" to stop)\\n')\n",
    "            if userText.lower() != 'quit':\n",
    "                GetLanguage(userText)\n",
    "\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "def GetLanguage(text):\n",
    "    try:\n",
    "        # Construct the JSON request body (a collection of documents, each with an ID and text)\n",
    "        jsonBody = {\n",
    "            \"documents\":[\n",
    "                {\"id\": 1,\n",
    "                 \"text\": text}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Let's take a look at the JSON we'll send to the service\n",
    "        print(json.dumps(jsonBody, indent=2))\n",
    "\n",
    "        # Make an HTTP request to the REST interface\n",
    "        uri = cog_endpoint.rstrip('/').replace('https://', '')\n",
    "        conn = http.client.HTTPSConnection(uri)\n",
    "\n",
    "        # Add the authentication key to the request header\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Ocp-Apim-Subscription-Key': cog_key\n",
    "        }\n",
    "\n",
    "        # Use the Text Analytics language API\n",
    "        conn.request(\"POST\", \"/text/analytics/v3.1/languages?\", str(jsonBody).encode('utf-8'), headers)\n",
    "\n",
    "        # Send the request\n",
    "        response = conn.getresponse()\n",
    "        data = response.read().decode(\"UTF-8\")\n",
    "\n",
    "        # If the call was successful, get the response\n",
    "        if response.status == 200:\n",
    "\n",
    "            # Display the JSON response in full (just so we can see it)\n",
    "            results = json.loads(data)\n",
    "            print(json.dumps(results, indent=2))\n",
    "\n",
    "            # Extract the detected language name for each document\n",
    "            for document in results[\"documents\"]:\n",
    "                print(\"\\nLanguage:\", document[\"detectedLanguage\"][\"name\"])\n",
    "\n",
    "        else:\n",
    "            # Something went wrong, write the whole response\n",
    "            print(data)\n",
    "\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Output:\n",
    "\n",
    "Enter some text (\"quit\" to stop)\n",
    "hello\n",
    "{\n",
    "  \"documents\": [\n",
    "    {\n",
    "      \"id\": 1,\n",
    "      \"text\": \"hello\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "{\n",
    "  \"documents\": [\n",
    "    {\n",
    "      \"id\": \"1\",\n",
    "      \"detectedLanguage\": {\n",
    "        \"name\": \"English\",\n",
    "        \"iso6391Name\": \"en\",\n",
    "        \"confidenceScore\": 1.0\n",
    "      },\n",
    "      \"warnings\": []\n",
    "    }\n",
    "  ],\n",
    "  \"errors\": [],\n",
    "  \"modelVersion\": \"2021-11-20\"\n",
    "}\n",
    "\n",
    "Language: English\n",
    "Enter some text (\"quit\" to stop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Using SDK to consume cognitive service</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute: pip install azure-ai-textanalytics==5.1.0\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "def main():\n",
    "    global cog_endpoint\n",
    "    global cog_key\n",
    "\n",
    "    try:\n",
    "        # Get Configuration Settings\n",
    "        load_dotenv()\n",
    "        cog_endpoint = os.getenv('COG_SERVICE_ENDPOINT')\n",
    "        cog_key = os.getenv('COG_SERVICE_KEY')\n",
    "\n",
    "        # Get user input (until they enter \"quit\")\n",
    "        userText =''\n",
    "        while userText.lower() != 'quit':\n",
    "            userText = input('\\nEnter some text (\"quit\" to stop)\\n')\n",
    "            if userText.lower() != 'quit':\n",
    "                language = GetLanguage(userText)\n",
    "                print('Language:', language)\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    \n",
    "def GetLanguage(text):\n",
    "\n",
    "    # Create client using endpoint and key\n",
    "    credential = AzureKeyCredential(cog_key)\n",
    "    client = TextAnalyticsClient(endpoint=cog_endpoint, credential=credential)\n",
    "\n",
    "    # Call the service to get the detected language\n",
    "    detectedLanguage = client.detect_language(documents = [text])[0]\n",
    "    return detectedLanguage.primary_language.name\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Output:\n",
    "    \n",
    "Enter some text (\"quit\" to stop)\n",
    "hasta la vista\n",
    "Language: Spanish\n",
    "\n",
    "Enter some text (\"quit\" to stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Secure congitve services</h3>\n",
    "\n",
    "You can store the subscription keys for a cognitive services resource in Azure Key Vault, and assign a managed identity to client applications that need to use the service. The applications can then retrieve the key as needed from the key vault, without risk of exposing it to unauthorized users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Token-based authentication</h3>\n",
    "\n",
    "When using the REST interface, some Cognitive Services support (or even require) token-based authentication. In these cases, the subscription key is presented in an initial request to obtain an authentication token, which has a valid period of 10 minutes. Subsequent requests must present the token to validate that the caller has been authenticated.\n",
    "\n",
    "<h3>Azure Active Directory authentication</h3>\n",
    "\n",
    "Some Cognitive Services support Azure Active Directory authentication, enabling you to grant access to specific service principals or managed identities for apps and services running in Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Cognitive Services are accessible from all networks. Some individual Cognitive Services resources (such as Text Analytics, Face, Computer Vision, and others) can be configured to restrict access to specific network addresses - either public Internet addresses or addresses on virtual networks."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Secure key access with Azure Key Vault\n",
    "You can develop applications that consume cognitive services by using a key for authentication. However, this means that the application code must be able to obtain the key. One option is to store the key in an environment variable or a configuration file where the application is deployed, but this approach leaves the key vulnerable to unauthorized access. A better approach when developing applications on Azure is to store the key securely in Azure Key Vault, and provide access to the key through a managed identity (in other words, a user account used by the application itself).\n",
    "\n",
    "Create a key vault and add a secret\n",
    "First, you need to create a key vault and add a secret for the cognitive services key.\n",
    "\n",
    "Make a note of the key1 value for your cognitive services resource (or copy it to the clipboard).\n",
    "In the Azure portal, on the Home page, select the ＋Create a resource button, search for Key Vault, and create a Key Vault resource with the following settings:\n",
    "Subscription: Your Azure subscription\n",
    "Resource group: The same resource group as your cognitive service resource\n",
    "Key vault name: Enter a unique name\n",
    "Region: The same region as your cognitive service resource\n",
    "Pricing tier: Standard\n",
    "Wait for deployment to complete and then go to your key vault resource.\n",
    "In the left navigation pane, select Secrets (in the Settings section).\n",
    "Select + Generate/Import and add a new secret with the following settings :\n",
    "Upload options: Manual\n",
    "Name: Cognitive-Services-Key (it's important to match this exactly, because later you'll run code that retrieves the secret based on this name)\n",
    "Value: Your key1 cognitive services key"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create a service principal\n",
    "To access the secret in the key vault, your application must use a service principal that has access to the secret. You'll use the Azure command line interface (CLI) to create the service principal, find its object ID, and grant access to the secret in Azure Vault.\n",
    "\n",
    "Return to Visual Studio Code, and in the integrated terminal for the 02-cognitive-security folder, run the following Azure CLI command, replacing <spName> with a suitable name for an application identity (for example, ai-app). Also replace <subscriptionId> and <resourceGroup> with the correct values for your subscription ID and the resource group containing your cognitive services and key vault resources:\n",
    "\n",
    "Tip: If you are unsure of your subscription ID, use the az account show command to retrieve your subscription information - the subscription ID is the id attribute in the output.\n",
    "\n",
    "az ad sp create-for-rbac -n \"api://<spName>\" --role owner --scopes subscriptions/<subscriptionId>/resourceGroups/<resourceGroup>\n",
    "The output of this command includes information about your new service principal. It should look similar to this:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"appId\": \"abcd12345efghi67890jklmn\",\n",
    "    \"displayName\": \"ai-app\",\n",
    "    \"name\": \"http://ai-app\",\n",
    "    \"password\": \"1a2b3c4d5e6f7g8h9i0j\",\n",
    "    \"tenant\": \"1234abcd5678fghi90jklm\"\n",
    "}\n",
    "```\n",
    "Make a note of the appId, password, and tenant values - you will need them later (if you close this terminal, you won't be able to retrieve the password; so it's important to note the values now - you can paste the output into a new text file in Visual Studio Code to ensure you can find the values you need later!)\n",
    "\n",
    "To get the object ID of your service principal, run the following Azure CLI command, replacing <appId> with the value of your service principal's app ID.\n",
    "\n",
    "az ad sp show --id <appId> --query objectId --out tsv\n",
    "To assign permission for your new service principal to access secrets in your Key Vault, run the following Azure CLI command, replacing <keyVaultName> with the name of your Azure Key Vault resource and <objectId> with the value of your service principal's object ID.\n",
    "\n",
    "az keyvault set-policy -n <keyVaultName> --object-id <objectId> --secret-permissions get list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ce088231c966>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#keyvault-client.py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mazure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextanalytics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextAnalyticsClient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "#keyvault-client.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from azure.identity import ClientSecretCredential\n",
    "\n",
    "\n",
    "def main():\n",
    "    global cog_endpoint\n",
    "    global cog_key\n",
    "\n",
    "    try:\n",
    "        # Get Configuration Settings\n",
    "        load_dotenv()\n",
    "        cog_endpoint = os.getenv('COG_SERVICE_ENDPOINT')\n",
    "        key_vault_name = os.getenv('KEY_VAULT')\n",
    "        app_tenant = os.getenv('TENANT_ID')\n",
    "        app_id = os.getenv('APP_ID')\n",
    "        app_password = os.getenv('APP_PASSWORD')\n",
    "\n",
    "        # Get cognitive services key from keyvault using the service principal credentials\n",
    "        key_vault_uri = f\"https://{key_vault_name}.vault.azure.net/\"\n",
    "        credential = ClientSecretCredential(app_tenant, app_id, app_password)\n",
    "        keyvault_client = SecretClient(key_vault_uri, credential)\n",
    "        secret_key = keyvault_client.get_secret(\"Cognitive-Services-Key\")\n",
    "        cog_key = secret_key.value\n",
    "\n",
    "        # Get user input (until they enter \"quit\")\n",
    "        userText =''\n",
    "        while userText.lower() != 'quit':\n",
    "            userText = input('\\nEnter some text (\"quit\" to stop)\\n')\n",
    "            if userText.lower() != 'quit':\n",
    "                language = GetLanguage(userText)\n",
    "                print('Language:', language)\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "def GetLanguage(text):\n",
    "\n",
    "    # Create client using endpoint and key\n",
    "    credential = AzureKeyCredential(cog_key)\n",
    "    client = TextAnalyticsClient(endpoint=cog_endpoint, credential=credential)\n",
    "\n",
    "    # Call the service to get the detected language\n",
    "    detectedLanguage = client.detect_language(documents = [text])[0]\n",
    "    return detectedLanguage.primary_language.name\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Monitor Cognitive Service</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Monitor cost</h3>\n",
    "\n",
    "Plan costs for Cognitive Services\n",
    "\n",
    "Before deploying a solution that depends on Cognitive Services, you can estimate costs by using the Azure Pricing Calculator.\n",
    "\n",
    "To view costs for Cognitive Services, sign into the Azure portal and select your subscription. You can then view overall costs for the subscription by selecting the Cost analysis tab. To view only costs for Cognitive Services, add a filter that restricts the data to reflect resources with a service name of azure cognitive services.\n",
    "\n",
    "<h3>Alert rules</h3>\n",
    "To create an alert rule for a Cognitive Services resource, select the resource in the Azure portal and on the Alerts tab, add a new alert rule. To define the alert rule, you must specify:\n",
    "\n",
    "The scope of the alert rule - in other words, the resource you want to monitor.\n",
    "A condition on which the alert is triggered. The specific trigger for the alert is based on a signal type, which can be Activity Log (an entry in the activity log created by an action performed on the resource, such as regenerating its subscription keys) or Metric (a metric threshold such as the number of errors exceeding 10 in an hour).\n",
    "Optional actions, such as sending an email to an administrator notifying them of the alert, or running an Azure Logic App to address the issue automatically.\n",
    "Alert rule details, such as a name for the alert rule and the resource group in which it should be defined.\n",
    "\n",
    "You can view metrics for an individual resource in the Azure portal by selecting the resource and viewing its Metrics page. On this page, you can add resource-specific metrics to charts. By default an empty chart is created for you, and you can add more charts as required.\n",
    "\n",
    "For example, the following image shows the Metrics page for an Azure Cognitive Services resource, showing the count of total calls to the service over a period of time.\n",
    "\n",
    "In the Azure portal, you can create dashboards that consist of multiple visualizations from different resources in your Azure environment to help you gain an overall view of the health and performance of your Azure resources.\n",
    "\n",
    "To create a dashboard, select Dashboard in the Azure portal menu (your default view may already be set to a dashboard rather than the portal home page). From here, you can add up to 100 named dashboards to encapsulate views for specific aspects of your Azure services that you want to track.\n",
    "\n",
    "<h3>Manage diagnostic logging</h3>\n",
    "\n",
    "To capture diagnostic logs for a Cognitive Services resource, you need a destination for the log data. You can use Azure Event Hub as a destination in order to then forward the data on to a custom telemetry solution, and you can connect directly to some third-party solutions; but in most cases you'll use one (or both) of the following kinds of resource within your Azure subscription:\n",
    "\n",
    "    Azure Log Analytics - a service that enables you to query and visualize log data within the Azure portal.\n",
    "    Azure Storage - a cloud-based data store that you can use to store log archives (which can be exported for analysis in other tools as needed).\n",
    "You should create these resources before configuring diagnostic logging for your Cognitive Services resource. If you intend to archive log data to Azure Storage, create the Azure Storage account in the same region as your Cognitive Services resource.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Use Cognitive Services containers</h3>\n",
    "\n",
    "To deploy and use a Cognitive Services container, the following three activities must occur:\n",
    "\n",
    "The container image for the specific Cognitive Services API you want to use is downloaded and deployed to a container host, such as a local Docker server, an Azure Container Instance (ACI), or Azure Kubernetes Service (AKS).\n",
    "Client applications submit data to the endpoint provided by the containerized service, and retrieve results just as they would from a Cognitive Services cloud resource in Azure.\n",
    "Periodically, usage metrics for the containerized service are sent to a Cognitive Services resource in Azure in order to calculate billing for the service.\n",
    "\n",
    "Even when using a container, you must provision a Cognitive Services resource in Azure for billing purposes\n",
    "\n",
    "When you deploy a Cognitive Services container image to a host, you must specify three settings.\n",
    "\n",
    "Setting\tDescription<br>\n",
    "ApiKey\tKey from your deployed Azure Cognitive Service; used for billing.<br>\n",
    "Billing\tEndpoint URI from your deployed Azure Cognitive Service; used for billing.<br>\n",
    "Eula\tValue of accept to state you accept the license for the container."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Deploy and run a Text Analytics container\n",
    "Many commonly used cognitive services APIs are available in container images. For a full list, check out the cognitive services documentation. In this exercise, you'll use the container image for the Text Analytics language detection API; but the principles are the same for all of the available images.\n",
    "\n",
    "In the Azure portal, on the Home page, select the ＋Create a resource button, search for container instances, and create a Container Instances resource with the following settings:\n",
    "\n",
    "Basics:\n",
    "\n",
    "Subscription: Your Azure subscription\n",
    "Resource group: Choose the resource group containing your cognitive services resource\n",
    "Container name: Enter a unique name\n",
    "Region: Choose any available region\n",
    "Image source: Docker Hub or other Registry\n",
    "Image type: Public\n",
    "Image: mcr.microsoft.com/azure-cognitive-services/textanalytics/language:1.1.013570001-amd64\n",
    "OS type: Linux\n",
    "Size: 1 vcpu, 4 GB memory\n",
    "Networking:\n",
    "\n",
    "Networking type: Public\n",
    "DNS name label: Enter a unique name for the container endpoint\n",
    "Ports: Change the TCP port from 80 to 5000\n",
    "Advanced:\n",
    "\n",
    "Restart policy: On failure\n",
    "\n",
    "Environment variables:\n",
    "\n",
    "Mark as secure\tKey\tValue\n",
    "Yes\tApiKey\tEither key for your cognitive services resource\n",
    "Yes\tBilling\tThe endpoint URI for your cognitive services resource\n",
    "No\tEula\taccept\n",
    "Command override: [ ]\n",
    "\n",
    "Tags:\n",
    "\n",
    "Don't add any tags\n",
    "Wait for deployment to complete, and then go to the deployed resource.\n",
    "\n",
    "Observe the following properties of your container instance resource on its Overview page:\n",
    "\n",
    "Status: This should be Running.\n",
    "IP Address: This is the public IP address you can use to access your container instances.\n",
    "FQDN: This is the fully-qualified domain name of the container instances resource, you can use this to access the container instances instead of the IP address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "command to run container cognitive service\n",
    "\n",
    "curl -X POST \"http://<your_ACI_IP_address_or_FQDN>:5000/text/analytics/v3.0/languages?\" -H \"Content-Type: application/json\" --data-ascii \"{'documents':[{'id':1,'text':'Hello world.'},{'id':2,'text':'Salut tout le monde.'}]}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Process and translate text with ACS </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Language service is designed to help you extract information from text. It provides functionality that you can use for:\n",
    "\n",
    "1. Language detection - determining the language in which text is written.\n",
    "2. Key phrase extraction - identifying important words and phrases in the text that indicate the main points.\n",
    "3. Sentiment analysis - quantifying how positive or negative the text is.\n",
    "4. Named entity recognition - detecting references to entities, including people, locations, time periods, organizations, and more.\n",
    "5. Entity linking - identifying specific entities by providing reference links to Wikipedia articles.\n",
    "\n",
    "To use the Language service to analyze text, you must provision a resource for it in your Azure subscription. You can provision a single-service Language resource, or you can use a multi-service Cognitive Services resource.\n",
    "\n",
    "The Language service recognizes up to 120 languages.\n",
    "\n",
    "Language detection can work with documents or single phrases. It's important to note that the document size must be under 5,120 characters. The size limit is per document and each collection is restricted to 1,000 items (IDs). A sample of a properly formatted JSON payload that you might submit to the service in the request body is shown here, including a collection of documents, each containing a unique id and the text to be analyzed. Optionally, you can provide a countryHint to improve prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'countryHint': 'US', 'id': '1', 'text': 'Hello world'},\n",
       "  {'id': '2', 'text': 'Bonjour tout le monde'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"documents\": [\n",
    "    {\n",
    "      \"countryHint\": \"US\",\n",
    "      \"id\": \"1\",\n",
    "      \"text\": \"Hello world\"\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"2\",\n",
    "      \"text\": \"Bonjour tout le monde\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The service will return a JSON response that contains a result for each document in the request body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'id': '1',\n",
       "   'detectedLanguage': {'name': 'English',\n",
       "    'iso6391Name': 'en',\n",
       "    'confidenceScore': 1},\n",
       "   'warnings': []},\n",
       "  {'id': '2',\n",
       "   'detectedLanguage': {'name': 'French',\n",
       "    'iso6391Name': 'fr',\n",
       "    'confidenceScore': 1},\n",
       "   'warnings': []}],\n",
       " 'errors': [],\n",
       " 'modelVersion': '2020-04-01'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"documents\": [\n",
    "   {\n",
    "     \"id\": \"1\",\n",
    "     \"detectedLanguage\": {\n",
    "       \"name\": \"English\",\n",
    "       \"iso6391Name\": \"en\",\n",
    "       \"confidenceScore\": 1\n",
    "     },\n",
    "     \"warnings\": []\n",
    "   },\n",
    "   {\n",
    "     \"id\": \"2\",\n",
    "     \"detectedLanguage\": {\n",
    "       \"name\": \"French\",\n",
    "       \"iso6391Name\": \"fr\",\n",
    "       \"confidenceScore\": 1\n",
    "     },\n",
    "     \"warnings\": []\n",
    "   }\n",
    "  ],\n",
    "  \"errors\": [],\n",
    "  \"modelVersion\": \"2020-04-01\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Mixed language content within the same document returns the language with the largest representation in the content, but with a lower positive rating, reflecting the marginal strength of that assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'id': '1',\n",
       "   'text': 'Hello, I would like to take a class at your University. ¿Se ofrecen clases en español? Es mi primera lengua y más fácil para escribir. Que diriez-vous des cours en français?'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"documents\": [\n",
    "    {\n",
    "      \"id\": \"1\",\n",
    "      \"text\": \"Hello, I would like to take a class at your University. ¿Se ofrecen clases en español? Es mi primera lengua y más fácil para escribir. Que diriez-vous des cours en français?\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'id': '1',\n",
       "   'detectedLanguages': [{'name': 'Spanish',\n",
       "     'iso6391Name': 'es',\n",
       "     'score': 0.9375}]}],\n",
       " 'errors': []}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"documents\": [\n",
    "    {\n",
    "      \"id\": \"1\",\n",
    "      \"detectedLanguages\": [\n",
    "        {\n",
    "          \"name\": \"Spanish\",\n",
    "          \"iso6391Name\": \"es\",\n",
    "          \"score\": 0.9375\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"errors\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last condition to consider is when there is ambiguity as to the language content. The scenario might happen if you submit textual content that the analyzer is not able to parse, for example because of character encoding issues when converting the text to a string variable. As a result, the response for the language name and ISO code will indicate (unknown) and the score value will be returned as NaN, or Not a Number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extract key phrases</h3>\n",
    "\n",
    "Key phrase extraction is the process of evaluating the text of a document, or documents, and then identifying the main points around the context of the document(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'id': '1',\n",
       "   'language': 'en',\n",
       "   'text': 'You must be the change you wish to see in the world.'},\n",
       "  {'id': '2',\n",
       "   'language': 'en',\n",
       "   'text': 'The journey of a thousand miles begins with a single step.'}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"documents\": [\n",
    "    {\n",
    "      \"id\": \"1\",\n",
    "      \"language\": \"en\",\n",
    "      \"text\": \"You must be the change you wish to see in the world.\"\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"2\",\n",
    "      \"language\": \"en\",\n",
    "      \"text\": \"The journey of a thousand miles begins with a single step.\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'id': '1', 'keyPhrases': ['change', 'world'], 'warnings': []},\n",
       "  {'id': '2',\n",
       "   'keyPhrases': ['miles', 'single step', 'journey'],\n",
       "   'warnings': []}],\n",
       " 'errors': [],\n",
       " 'modelVersion': '2020-04-01'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"documents\": [\n",
    "   {\n",
    "     \"id\": \"1\",\n",
    "     \"keyPhrases\": [\n",
    "       \"change\",\n",
    "       \"world\"\n",
    "     ],\n",
    "     \"warnings\": []\n",
    "   },\n",
    "   {\n",
    "     \"id\": \"2\",\n",
    "     \"keyPhrases\": [\n",
    "       \"miles\",\n",
    "       \"single step\",\n",
    "       \"journey\"\n",
    "     ],\n",
    "     \"warnings\": []\n",
    "   }\n",
    "  ],\n",
    "  \"errors\": [],\n",
    "  \"modelVersion\": \"2020-04-01\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Analyze Sentiment</h3>\n",
    "\n",
    "When using the Language service to evaluate sentiment, the response includes overall document sentiment and individual sentence sentiment for each document submitted to the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'language': 'en', 'id': '1', 'text': 'Smile! Life is good!'}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"documents\": [\n",
    "    {\n",
    "      \"language\": \"en\",\n",
    "      \"id\": \"1\",\n",
    "      \"text\": \"Smile! Life is good!\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'id': '1',\n",
       "   'sentiment': 'positive',\n",
       "   'confidenceScores': {'positive': 0.99, 'neutral': 0.01, 'negative': 0.0},\n",
       "   'sentences': [{'text': 'Smile!',\n",
       "     'sentiment': 'positive',\n",
       "     'confidenceScores': {'positive': 0.97, 'neutral': 0.02, 'negative': 0.01},\n",
       "     'offset': 0,\n",
       "     'length': 6},\n",
       "    {'text': 'Life is good!',\n",
       "     'sentiment': 'positive',\n",
       "     'confidenceScores': {'positive': 0.98, 'neutral': 0.02, 'negative': 0.0},\n",
       "     'offset': 7,\n",
       "     'length': 13}],\n",
       "   'warnings': []}],\n",
       " 'errors': [],\n",
       " 'modelVersion': '2020-04-01'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"documents\": [\n",
    "   {\n",
    "     \"id\": \"1\",\n",
    "     \"sentiment\": \"positive\",\n",
    "     \"confidenceScores\": {\n",
    "       \"positive\": 0.99,\n",
    "       \"neutral\": 0.01,\n",
    "       \"negative\": 0.00\n",
    "     },\n",
    "     \"sentences\": [\n",
    "       {\n",
    "         \"text\": \"Smile!\",\n",
    "         \"sentiment\": \"positive\",\n",
    "         \"confidenceScores\": {   \n",
    "             \"positive\": 0.97,\n",
    "\t         \"neutral\": 0.02, \n",
    "             \"negative\": 0.01\n",
    "           },\n",
    "         \"offset\": 0,\n",
    "         \"length\": 6\n",
    "       },\n",
    "       {\n",
    "\t      \"text\": \"Life is good!\",\n",
    "          \"sentiment\": \"positive\",\n",
    "          \"confidenceScores\": {   \n",
    "             \"positive\": 0.98,\n",
    "\t         \"neutral\": 0.02,  \n",
    "             \"negative\": 0.00\n",
    "           },\n",
    "         \"offset\": 7,\n",
    "         \"length\": 13\n",
    "       }\n",
    "     ],\n",
    "     \"warnings\": []\n",
    "   }\n",
    "  ],\n",
    "  \"errors\": [],\n",
    "  \"modelVersion\": \"2020-04-01\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall document sentiment is based on sentences:\n",
    "\n",
    "    If all sentences are neutral, the overall sentiment is neutral.\n",
    "    If sentence classifications include only positive and neutral, the overall sentiment is positive.\n",
    "    If the sentence classifications include only negative and neutral, the overall sentiment is negative.\n",
    "    If the sentence classifications include positive and negative, the overall sentiment is mixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extract entities</h3>\n",
    "\n",
    "Named Entity Recognition identifies entities that are mentioned in the text. Entities are grouped into categories and subcategories, for example:\n",
    "\n",
    "    Person\n",
    "    Location\n",
    "    DateTime\n",
    "    Organization\n",
    "    Address\n",
    "    Email\n",
    "    URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'language': 'en',\n",
       "   'id': '1',\n",
       "   'text': 'Joe went to London on Saturday'}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"documents\": [\n",
    "    {\n",
    "      \"language\": \"en\",\n",
    "      \"id\": \"1\",\n",
    "      \"text\": \"Joe went to London on Saturday\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'id': '1',\n",
       "   'entities': [{'text': 'Joe',\n",
       "     'category': 'Person',\n",
       "     'offset': 0,\n",
       "     'length': 3,\n",
       "     'confidenceScore': 0.62},\n",
       "    {'text': 'London',\n",
       "     'category': 'Location',\n",
       "     'subcategory': 'GPE',\n",
       "     'offset': 12,\n",
       "     'length': 6,\n",
       "     'confidenceScore': 0.88},\n",
       "    {'text': 'Saturday',\n",
       "     'category': 'DateTime',\n",
       "     'subcategory': 'Date',\n",
       "     'offset': 22,\n",
       "     'length': 8,\n",
       "     'confidenceScore': 0.8}],\n",
       "   'warnings': []}],\n",
       " 'errors': [],\n",
       " 'modelVersion': '2021-01-15'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"documents\":[\n",
    "      {\n",
    "          \"id\":\"1\",\n",
    "          \"entities\":[\n",
    "          {\n",
    "            \"text\":\"Joe\",\n",
    "            \"category\":\"Person\",\n",
    "            \"offset\":0,\n",
    "            \"length\":3,\n",
    "            \"confidenceScore\":0.62\n",
    "          },\n",
    "          {\n",
    "            \"text\":\"London\",\n",
    "            \"category\":\"Location\",\n",
    "            \"subcategory\":\"GPE\",\n",
    "            \"offset\":12,\n",
    "            \"length\":6,\n",
    "            \"confidenceScore\":0.88\n",
    "          },\n",
    "          {\n",
    "            \"text\":\"Saturday\",\n",
    "            \"category\":\"DateTime\",\n",
    "            \"subcategory\":\"Date\",\n",
    "            \"offset\":22,\n",
    "            \"length\":8,\n",
    "            \"confidenceScore\":0.8\n",
    "          }\n",
    "        ],\n",
    "        \"warnings\":[]\n",
    "      }\n",
    "  ],\n",
    "  \"errors\":[],\n",
    "  \"modelVersion\":\"2021-01-15\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extract linked entities</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity linking can be used to disambiguate entities of the same name by referencing an article in a knowledge base. Wikipedia provides the knowledge base for the Text Analytics service. Specific article links are determined based on entity context within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'language': 'en',\n",
       "   'id': '1',\n",
       "   'text': 'I saw Venus shining in the sky'}]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"documents\": [\n",
    "    {\n",
    "      \"language\": \"en\",\n",
    "      \"id\": \"1\",\n",
    "      \"text\": \"I saw Venus shining in the sky\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'id': '1',\n",
       "   'entities': [{'name': 'Venus',\n",
       "     'matches': [{'text': 'Venus',\n",
       "       'offset': 6,\n",
       "       'length': 5,\n",
       "       'confidenceScore': 0.01}],\n",
       "     'language': 'en',\n",
       "     'id': 'Venus',\n",
       "     'url': 'https://en.wikipedia.org/wiki/Venus',\n",
       "     'dataSource': 'Wikipedia'}],\n",
       "   'warnings': []}],\n",
       " 'errors': [],\n",
       " 'modelVersion': '2020-02-01'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"documents\":\n",
    "    [\n",
    "      {\n",
    "        \"id\":\"1\",\n",
    "        \"entities\":[\n",
    "          {\n",
    "            \"name\":\"Venus\",\n",
    "            \"matches\":[\n",
    "              {\n",
    "                \"text\":\"Venus\",\n",
    "                \"offset\":6,\n",
    "                \"length\":5,\n",
    "                \"confidenceScore\":0.01\n",
    "              }\n",
    "            ],\n",
    "            \"language\":\"en\",\n",
    "            \"id\":\"Venus\",\n",
    "            \"url\":\"https://en.wikipedia.org/wiki/Venus\",\n",
    "            \"dataSource\":\"Wikipedia\"\n",
    "          }\n",
    "        ],\n",
    "        \"warnings\":[]\n",
    "      }\n",
    "    ],\n",
    "  \"errors\":[],\n",
    "  \"modelVersion\":\"2020-02-01\"\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Provision a Cognitive Services resource\n",
    "If you don't already have one in your subscription, you'll need to provision a Cognitive Services resource.\n",
    "\n",
    "Open the Azure portal at https://portal.azure.com, and sign in using the Microsoft account associated with your Azure subscription.\n",
    "Select the ＋Create a resource button, search for cognitive services, and create a Cognitive Services resource with the following settings:\n",
    "Subscription: Your Azure subscription\n",
    "Resource group: Choose or create a resource group (if you are using a restricted subscription, you may not have permission to create a new resource group - use the one provided)\n",
    "Region: Choose any available region\n",
    "Name: Enter a unique name\n",
    "Pricing tier: Standard S0\n",
    "Select the required checkboxes and create the resource.\n",
    "Wait for deployment to complete, and then view the deployment details.\n",
    "When the resource has been deployed, go to it and view its Keys and Endpoint page. You will need the endpoint and one of the keys from this page in the next procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install azure-ai-textanalytics==5.1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-f6f6e4dadcf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Import namespaces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# import namespaces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Import namespaces\n",
    "# import namespaces\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Get Configuration Settings\n",
    "        load_dotenv()\n",
    "        cog_endpoint = os.getenv('COG_SERVICE_ENDPOINT')\n",
    "        cog_key = os.getenv('COG_SERVICE_KEY')\n",
    "\n",
    "        # Create client using endpoint and key\n",
    "        # Create client using endpoint and key\n",
    "        credential = AzureKeyCredential(cog_key)\n",
    "        cog_client = TextAnalyticsClient(endpoint=cog_endpoint, credential=credential)\n",
    "\n",
    "        # Analyze each text file in the reviews folder\n",
    "        reviews_folder = 'reviews'\n",
    "        for file_name in os.listdir(reviews_folder):\n",
    "            # Read the file contents\n",
    "            print('\\n-------------\\n' + file_name)\n",
    "            text = open(os.path.join(reviews_folder, file_name), encoding='utf8').read()\n",
    "            print('\\n' + text)\n",
    "\n",
    "            # Get language\n",
    "            # Get language\n",
    "            detectedLanguage = cog_client.detect_language(documents=[text])[0]\n",
    "            print('\\nLanguage: {}'.format(detectedLanguage.primary_language.name))\n",
    "\n",
    "            # Get sentiment\n",
    "            # Get sentiment\n",
    "            sentimentAnalysis = cog_client.analyze_sentiment(documents=[text])[0]\n",
    "            print(\"\\nSentiment: {}\".format(sentimentAnalysis.sentiment))\n",
    "\n",
    "            # Get key phrases\n",
    "            # Get key phrases\n",
    "            phrases = cog_client.extract_key_phrases(documents=[text])[0].key_phrases\n",
    "            if len(phrases) > 0:\n",
    "                print(\"\\nKey Phrases:\")\n",
    "                for phrase in phrases:\n",
    "                    print('\\t{}'.format(phrase))\n",
    "\n",
    "            # Get entities\n",
    "            # Get entities\n",
    "            entities = cog_client.recognize_entities(documents=[text])[0].entities\n",
    "            if len(entities) > 0:\n",
    "                print(\"\\nEntities\")\n",
    "                for entity in entities:\n",
    "                    print('\\t{} ({})'.format(entity.text, entity.category))\n",
    "\n",
    "            # Get linked entities\n",
    "            # Get linked entities\n",
    "            entities = cog_client.recognize_linked_entities(documents=[text])[0].entities\n",
    "            if len(entities) > 0:\n",
    "                print(\"\\nLinks\")\n",
    "                for linked_entity in entities:\n",
    "                    print('\\t{} ({})'.format(linked_entity.name, linked_entity.url))\n",
    "\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Output\n",
    "\n",
    "C:\\Users\\Student\\Documents\\AI-102-AIEngineer\\05-analyze-text\\Python\\text-analysis>python text-analysis.py\n",
    "\n",
    "-------------\n",
    "review1.txt\n",
    "\n",
    "Good Hotel and staff\n",
    "The Royal Hotel, London, UK\n",
    "3/2/2018\n",
    "Clean rooms, good service, great location near Buckingham Palace and Westminster Abbey, and so on. We thoroughly enjoyed our stay. The courtyard is very peaceful and we went to a restaurant which is part of the same group and is Indian ( West coast so plenty of fish) with a Michelin Star. We had the taster menu which was fabulous. The rooms were very well appointed with a kitchen, lounge, bedroom and enormous bathroom. Thoroughly recommended.\n",
    "\n",
    "Language: English\n",
    "\n",
    "Sentiment: positive\n",
    "\n",
    "Key Phrases:\n",
    "        The Royal Hotel\n",
    "        Good Hotel\n",
    "        good service\n",
    "        great location\n",
    "        Buckingham Palace\n",
    "        Westminster Abbey\n",
    "        same group\n",
    "        West coast\n",
    "        Michelin Star\n",
    "        taster menu\n",
    "        enormous bathroom\n",
    "        Clean rooms\n",
    "        staff\n",
    "        London\n",
    "        UK\n",
    "        stay\n",
    "        courtyard\n",
    "        restaurant\n",
    "        part\n",
    "        plenty\n",
    "        fish\n",
    "        kitchen\n",
    "        lounge\n",
    "        bedroom\n",
    "\n",
    "Entities\n",
    "        Hotel (Location)\n",
    "        staff (PersonType)\n",
    "        The Royal Hotel (Location)\n",
    "        London (Location)\n",
    "        UK (Location)\n",
    "        3/2/2018 (DateTime)\n",
    "        Buckingham Palace (Location)\n",
    "        Westminster Abbey (Location)\n",
    "        courtyard (Location)\n",
    "        restaurant (Location)\n",
    "        Indian (PersonType)\n",
    "        West coast (Location)\n",
    "        fish (Product)\n",
    "        Michelin (Product)\n",
    "        taster (PersonType)\n",
    "        rooms (Location)\n",
    "        kitchen (Location)\n",
    "        lounge (Location)\n",
    "        bedroom (Location)\n",
    "        bathroom (Location)\n",
    "\n",
    "Links\n",
    "        GOOD Music (https://en.wikipedia.org/wiki/GOOD_Music)\n",
    "        Hotel (https://en.wikipedia.org/wiki/Hotel)\n",
    "        The Royal Hotel (https://en.wikipedia.org/wiki/The_Royal_Hotel)\n",
    "        London (https://en.wikipedia.org/wiki/London)\n",
    "        Buckingham Palace (https://en.wikipedia.org/wiki/Buckingham_Palace)\n",
    "        Westminster Abbey (https://en.wikipedia.org/wiki/Westminster_Abbey)\n",
    "        India (https://en.wikipedia.org/wiki/India)\n",
    "        West Coast Main Line (https://en.wikipedia.org/wiki/West_Coast_Main_Line)\n",
    "        Michelin Guide (https://en.wikipedia.org/wiki/Michelin_Guide)\n",
    "\n",
    "-------------\n",
    "review2.txt\n",
    "\n",
    "Tired hotel with poor service\n",
    "The Royal Hotel, London, United Kingdom\n",
    "5/6/2018\n",
    "This is a old hotel (has been around since 1950's) and the room furnishings are average - becoming a bit old now and require changing. The internet didn't work and had to come to one of their office rooms to check in for my flight home. The website says it's close to the British Museum, but it's too far to walk.\n",
    "\n",
    "Language: English\n",
    "\n",
    "Sentiment: negative\n",
    "\n",
    "Key Phrases:\n",
    "        The Royal Hotel\n",
    "        Tired hotel\n",
    "        old hotel\n",
    "        poor service\n",
    "        United Kingdom\n",
    "        room furnishings\n",
    "        office rooms\n",
    "        flight home\n",
    "        British Museum\n",
    "        London\n",
    "        changing\n",
    "        internet\n",
    "        website\n",
    "        1950\n",
    "\n",
    "Entities\n",
    "        hotel (Location)\n",
    "        The Royal Hotel (Location)\n",
    "        London (Location)\n",
    "        United Kingdom (Location)\n",
    "        5/6/2018 (DateTime)\n",
    "        hotel (Location)\n",
    "        since 1950's (DateTime)\n",
    "        bit (Quantity)\n",
    "        now (DateTime)\n",
    "        one (Quantity)\n",
    "        office rooms (Location)\n",
    "        home (Location)\n",
    "        British Museum (Location)\n",
    "\n",
    "Links\n",
    "        The Royal Hotel (https://en.wikipedia.org/wiki/The_Royal_Hotel)\n",
    "        London (https://en.wikipedia.org/wiki/London)\n",
    "        British Museum (https://en.wikipedia.org/wiki/British_Museum)\n",
    "\n",
    "-------------\n",
    "review3.txt\n",
    "\n",
    "Good location and helpful staff, but on a busy road.\n",
    "The Lombard Hotel, San Francisco, USA\n",
    "8/16/2018\n",
    "We stayed here in August after reading reviews. We were very pleased with location, just behind Chestnut Street, a cosmopolitan and trendy area with plenty of restaurants to choose from. The    \n",
    "Marina district was lovely to wander through, very interesting houses. Make sure to walk to the San Francisco Museum of Fine Arts and the Marina to get a good view of Golden Gate bridge and the \n",
    "city. On a bus route and easy to get into centre. Rooms were clean with plenty of room and staff \n",
    "were friendly and helpful. The only down side was the noise from Lombard Street so ask to have a \n",
    "room furthest away from traffic noise.\n",
    "\n",
    "Language: English\n",
    "\n",
    "Sentiment: mixed\n",
    "\n",
    "Key Phrases:\n",
    "        Golden Gate bridge\n",
    "        The Lombard Hotel\n",
    "        The Marina district\n",
    "        San Francisco Museum\n",
    "        Lombard Street\n",
    "        busy road\n",
    "        Chestnut Street\n",
    "        trendy area\n",
    "        interesting houses\n",
    "        Fine Arts\n",
    "        good view\n",
    "        bus route\n",
    "        down side\n",
    "        Good location\n",
    "        helpful staff\n",
    "        traffic noise\n",
    "        USA\n",
    "        We\n",
    "        August\n",
    "        reviews\n",
    "        cosmopolitan\n",
    "        plenty\n",
    "        restaurants\n",
    "        city\n",
    "        centre\n",
    "        Rooms\n",
    "\n",
    "Entities\n",
    "        staff (PersonType)\n",
    "        The Lombard Hotel (Location)\n",
    "        San Francisco (Location)\n",
    "        USA (Location)\n",
    "        8/16/2018 (DateTime)\n",
    "        August (DateTime)\n",
    "        Chestnut Street (Address)\n",
    "        restaurants (Location)\n",
    "        Marina district (Location)\n",
    "        houses (Location)\n",
    "        San (Location)\n",
    "        Francisco Museum of Fine Arts (Location)\n",
    "        Marina (Location)\n",
    "        Golden Gate bridge (Location)\n",
    "        city (Location)\n",
    "        centre (Location)\n",
    "        Rooms (Location)\n",
    "        room (Location)\n",
    "        staff (PersonType)\n",
    "        Lombard Street (Address)\n",
    "        room (Location)\n",
    "\n",
    "Links\n",
    "        Lombardy (https://en.wikipedia.org/wiki/Lombardy)\n",
    "        Hotel (https://en.wikipedia.org/wiki/Hotel)\n",
    "        San Francisco (https://en.wikipedia.org/wiki/San_Francisco)\n",
    "        Chestnut Street (Philadelphia) (https://en.wikipedia.org/wiki/Chestnut_Street_(Philadelphia))\n",
    "        Marina District, San Francisco (https://en.wikipedia.org/wiki/Marina_District,_San_Francisco)\n",
    "        Museum of Fine Arts, Boston (https://en.wikipedia.org/wiki/Museum_of_Fine_Arts,_Boston)  \n",
    "        Golden Gate Bridge (https://en.wikipedia.org/wiki/Golden_Gate_Bridge)\n",
    "        Room (https://en.wikipedia.org/wiki/Room)\n",
    "        Lombard Street (San Francisco) (https://en.wikipedia.org/wiki/Lombard_Street_(San_Francisco))\n",
    "\n",
    "-------------\n",
    "review4.txt\n",
    "\n",
    "Very noisy and rooms are tiny\n",
    "The Lombard Hotel, San Francisco, USA\n",
    "9/5/2018\n",
    "Hotel is located on Lombard street which is a very busy SIX lane street directly off the Golden Gate Bridge. Traffic from early morning until late at night especially on weekends. Noise would not be so bad if rooms were better insulated but they are not. Had to put cotton balls in my ears to be able to sleep--was too tired to enjoy the city the next day. Rooms are TINY. I picked the room because it had two queen size beds--but the room barely had space to fit them. With family of \n",
    "four in the room it was tight. With all that said, rooms are clean and they've made an effort to \n",
    "update them. The hotel is in Marina district with lots of good places to eat, within walking distance to Presidio. May be good hotel for young stay-up-late adults on a budget\n",
    "\n",
    "\n",
    "Language: English\n",
    "\n",
    "Sentiment: mixed\n",
    "\n",
    "Key Phrases:\n",
    "        two queen size beds\n",
    "        busy SIX lane street\n",
    "        Golden Gate Bridge\n",
    "        The Lombard Hotel\n",
    "        Lombard street\n",
    "        San Francisco\n",
    "        early morning\n",
    "        cotton balls\n",
    "        Marina district\n",
    "        good places\n",
    "        walking distance\n",
    "        late adults\n",
    "        good hotel\n",
    "        rooms\n",
    "        USA\n",
    "        Traffic\n",
    "        night\n",
    "        weekends\n",
    "        Noise\n",
    "        ears\n",
    "        city\n",
    "        TINY\n",
    "        space\n",
    "        family\n",
    "        effort\n",
    "        lots\n",
    "        Presidio\n",
    "        young\n",
    "        budget\n",
    "\n",
    "Entities\n",
    "        rooms (Location)\n",
    "        The Lombard Hotel (Location)\n",
    "        San Francisco (Location)\n",
    "        USA (Location)\n",
    "        9/5/2018 (DateTime)\n",
    "        Hotel (Location)\n",
    "        Lombard street (Address)\n",
    "        SIX (Quantity)\n",
    "        Golden Gate Bridge (Location)\n",
    "        early morning (DateTime)\n",
    "        night (DateTime)\n",
    "        weekends (DateTime)\n",
    "        rooms (Location)\n",
    "        cotton balls (Product)\n",
    "        the next day (DateTime)\n",
    "        Rooms (Location)\n",
    "        room (Location)\n",
    "        two (Quantity)\n",
    "        beds (Product)\n",
    "        room (Location)\n",
    "        four in (Quantity)\n",
    "        room (Location)\n",
    "        rooms (Location)\n",
    "        hotel (Location)\n",
    "        Marina district (Location)\n",
    "        Presidio (Location)\n",
    "        hotel (Location)\n",
    "\n",
    "Links\n",
    "        Lombard, Illinois (https://en.wikipedia.org/wiki/Lombard,_Illinois)\n",
    "        Hotel (https://en.wikipedia.org/wiki/Hotel)\n",
    "        San Francisco (https://en.wikipedia.org/wiki/San_Francisco)\n",
    "        Lombard Street (San Francisco) (https://en.wikipedia.org/wiki/Lombard_Street_(San_Francisco))\n",
    "        Golden Gate Bridge (https://en.wikipedia.org/wiki/Golden_Gate_Bridge)\n",
    "        Traffic (https://en.wikipedia.org/wiki/Traffic)\n",
    "        Noise rock (https://en.wikipedia.org/wiki/Noise_rock)\n",
    "        Room (https://en.wikipedia.org/wiki/Room)\n",
    "        Marina District, San Francisco (https://en.wikipedia.org/wiki/Marina_District,_San_Francisco)\n",
    "        Presidio of San Francisco (https://en.wikipedia.org/wiki/Presidio_of_San_Francisco)      \n",
    "        May (https://en.wikipedia.org/wiki/May)\n",
    "\n",
    "-------------\n",
    "review5.txt\n",
    "\n",
    "Un hôtel agréable\n",
    "L'Hotel Buckingham, Londres, UK\n",
    "J’adore cet hôtel. Le personnel est très amical et les chambres sont confortables.\n",
    "\n",
    "Language: French\n",
    "\n",
    "Sentiment: positive\n",
    "\n",
    "Key Phrases:\n",
    "        hôtel agréable\n",
    "        L'Hotel Buckingham\n",
    "        Londres\n",
    "        UK\n",
    "        personnel\n",
    "        chambres\n",
    "\n",
    "Entities\n",
    "        hôtel (Location)\n",
    "        Hotel Buckingham (Location)\n",
    "        Londres (Location)\n",
    "        UK (Location)\n",
    "        hôtel (Location)\n",
    "        Le (Quantity)\n",
    "        personnel (PersonType)\n",
    "        chambres (Location)\n",
    "\n",
    "Links\n",
    "        United Nations (https://en.wikipedia.org/wiki/United_Nations)\n",
    "        L'Hôtel (https://en.wikipedia.org/wiki/L'Hôtel)\n",
    "        Buckingham (https://en.wikipedia.org/wiki/Buckingham)\n",
    "        London (https://en.wikipedia.org/wiki/London)\n",
    "        United Kingdom (https://en.wikipedia.org/wiki/United_Kingdom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Translator Azure Cognitive Service</h3>\n",
    "\n",
    "The Translator service provides a multilingual text translation API that you can use for:\n",
    "\n",
    "    Language detection\n",
    "    One-to-many translation\n",
    "    Script transliteration (converting text from its native script to an alternative script)\n",
    "    \n",
    "You can provision a single-service Translator resource, or you can use the Text Analytics API in a multi-service Cognitive Services resource.\n",
    "\n",
    "<h3>Language detection</h3>\n",
    "\n",
    "Use detect REST function to detect the language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text': 'こんにちは'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{ 'Text' : 'こんにちは' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-0eceb0f54933>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m [\n\u001b[0;32m      2\u001b[0m   {\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;34m\"isTranslationSupported\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;34m\"isTransliterationSupported\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;34m\"language\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"ja\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'true' is not defined"
     ]
    }
   ],
   "source": [
    "[\n",
    "  {\n",
    "    \"isTranslationSupported\": true,\n",
    "    \"isTransliterationSupported\": true,\n",
    "    \"language\": \"ja\",\n",
    "    \"score\": 1.0\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Translation</h3>\n",
    "\n",
    " use the translate function; specifying a single from parameter to indicate the source language, and one or more to parameters to specify the languages into which you want the text translated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translations': [{'text': 'Hello', 'to': 'en'},\n",
       "   {'text': 'Bonjour', 'to': 'fr'}]}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "  {\"translations\": \n",
    "    [\n",
    "      {\"text\": \"Hello\", \"to\": \"en\"},   \n",
    "      {\"text\": \"Bonjour\", \"to\": \"fr\"}\n",
    "    ]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Transliteration</h3>\n",
    "\n",
    "To accomplish this, we can submit the Japanese text to the transliterate function with a fromScript parameter of Jpan and a toScript parameter of Latn to get the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'script': 'Latn', 'text': \"Kon'nichiwa\"}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    {\n",
    "        \"script\": \"Latn\",\n",
    "        \"text\": \"Kon'nichiwa\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Word alignment</h3>\n",
    "\n",
    "In written English (using Latin script), spaces are used to separate words. However, in some other languages (and more specifically, scripts) this is not always the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resolve this problem, you can specify the includeAlignment parameter with a value of true to produce the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translations': [{'text': '认知服务',\n",
       "    'to': 'zh-Hans',\n",
       "    'alignment': {'proj': '0:8-0:1 10:17-2:3'}}]}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "  {\"translations\": \n",
    "    [\n",
    "      {\"text\": \"认知服务\", \"to\": \"zh-Hans\",\n",
    "       \"alignment\": {\"proj\": \"0:8-0:1 10:17-2:3\"}\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results tell us that characters 0 to 8 in the source correspond to characters 0 to 1 in the translation (), while characters 10 to 17 in the source correspond to characters 2 to 3 in the translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sentence length</h3>\n",
    "\n",
    "Sometimes it might be useful to know the length of a translation, for example to determine how best to display it in a user interface. You can get this information by setting the includeSentenceLength parameter to true"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For example, specifying this parameter when translating the English (en) text \"Hello world!\" to French (fr) produces the following results:\n",
    "\n",
    "[\n",
    "  {\"translations\": \n",
    "    [\n",
    "      {\"text\": \"Salut tout le monde!\", \"to\": \"fr\",\n",
    "       \"sentLen\":{\"srcSentLen\":[12], \"transSentLen\":[20]}\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Profanity filtering</h3>\n",
    "\n",
    "\n",
    "Sometimes text contains profanities, which you might want to obscure or omit altogether in a translation. You can handle profanities by specifying the profanityAction parameter, which can have one of the following values:\n",
    "\n",
    "    NoAction: Profanities are translated along with the rest of the text.\n",
    "    Deleted: Profanities are omitted in the translation.\n",
    "    Marked: Profanities are indicated using the technique indicated in the profanityMarker parameter (if supplied). The default value for this parameter is Asterisk, which replaces characters in profanities with \"*\". As an alternative, you can specify a profanityMarker value of Tag, which causes profanities to be enclosed in XML tags."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For example, translating the English (en) text \"JSON is ▇▇▇▇ great!\" (where the blocked out word is a profanity) to French (fr) with a profanityAction of Marked and a profanityMarker of Asterisk produces the following result:\n",
    "\n",
    "[\n",
    "  {\"translations\": \n",
    "    [\n",
    "      {\"text\": \"JSON est *** génial!\", \"to\": \"fr\"}\n",
    "    ]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define custom translations</h3>\n",
    "\n",
    "To solve this problem, you can create a custom model that maps your own sets of source and target terms for translation. To create a custom model, use the Custom Translator portal to:\n",
    "\n",
    "    Create a workspace linked to your Translator resource\n",
    "    Create a project\n",
    "    Upload training data files\n",
    "    Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your custom model is assigned a unique category Id, which you can specify in translate calls to your Translator resource by using the category parameter, causing translation to be performed by your custom model instead of the default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-de4ad96c929e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests, json\n",
    "\n",
    "def main():\n",
    "    global translator_endpoint\n",
    "    global cog_key\n",
    "    global cog_region\n",
    "\n",
    "    try:\n",
    "        # Get Configuration Settings\n",
    "        load_dotenv()\n",
    "        cog_key = os.getenv('COG_SERVICE_KEY')\n",
    "        cog_region = os.getenv('COG_SERVICE_REGION')\n",
    "        translator_endpoint = 'https://api.cognitive.microsofttranslator.com'\n",
    "\n",
    "        # Analyze each text file in the reviews folder\n",
    "        reviews_folder = 'reviews'\n",
    "        for file_name in os.listdir(reviews_folder):\n",
    "            # Read the file contents\n",
    "            print('\\n-------------\\n' + file_name)\n",
    "            text = open(os.path.join(reviews_folder, file_name), encoding='utf8').read()\n",
    "            print('\\n' + text)\n",
    "\n",
    "            # Detect the language\n",
    "            language = GetLanguage(text)\n",
    "            print('Language:',language)\n",
    "\n",
    "            # Translate if not already English\n",
    "            if language != 'en':\n",
    "                translation = Translate(text, language)\n",
    "                print(\"\\nTranslation:\\n{}\".format(translation))\n",
    "                \n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "def GetLanguage(text):\n",
    "    # Default language is English\n",
    "    language = 'en'\n",
    "\n",
    "    # Use the Translator detect function\n",
    "    # Use the Translator detect function\n",
    "    path = '/detect'\n",
    "    url = translator_endpoint + path\n",
    "\n",
    "    # Build the request\n",
    "    params = {\n",
    "        'api-version': '3.0'\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "    'Ocp-Apim-Subscription-Key': cog_key,\n",
    "    'Ocp-Apim-Subscription-Region': cog_region,\n",
    "    'Content-type': 'application/json'\n",
    "    }\n",
    "\n",
    "    body = [{\n",
    "        'text': text\n",
    "    }]\n",
    "\n",
    "    # Send the request and get response\n",
    "    request = requests.post(url, params=params, headers=headers, json=body)\n",
    "    response = request.json()\n",
    "\n",
    "    # Parse JSON array and get language\n",
    "    language = response[0][\"language\"]\n",
    "\n",
    "    # Return the language\n",
    "    return language\n",
    "\n",
    "def Translate(text, source_language):\n",
    "    translation = ''\n",
    "\n",
    "    # Use the Translator translate function\n",
    "    # Use the Translator translate function\n",
    "    path = '/translate'\n",
    "    url = translator_endpoint + path\n",
    "\n",
    "    # Build the request\n",
    "    params = {\n",
    "        'api-version': '3.0',\n",
    "        'from': source_language,\n",
    "        'to': ['en']\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': cog_key,\n",
    "        'Ocp-Apim-Subscription-Region': cog_region,\n",
    "        'Content-type': 'application/json'\n",
    "    }\n",
    "\n",
    "    body = [{\n",
    "        'text': text\n",
    "    }]\n",
    "\n",
    "    # Send the request and get response\n",
    "    request = requests.post(url, params=params, headers=headers, json=body)\n",
    "    response = request.json()\n",
    "\n",
    "    # Parse JSON array and get translation\n",
    "    translation = response[0][\"translations\"][0][\"text\"]\n",
    "\n",
    "    # Return the translation\n",
    "    return translation\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Speech enabled apps with speech service</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Speech service provides APIs that you can use to build speech-enabled applications. Specifically, the Speech service supports:\n",
    "\n",
    "    Speech-to-Text: An API that enables speech recognition in which your application can accept spoken input.\n",
    "    Text-to-Speech: An API that enables speech synthesis in which your application can provide spoken output.\n",
    "    Speech Translation: An API that you can use to translate spoken input into multiple languages.\n",
    "    Speaker Recognition: An API that enables your application to recognize individual speakers based on their voice.\n",
    "    Intent Recognition: An API that integrates with the Language Understanding service to determine the semantic meaning of spoken input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You can use either a dedicated Speech resource or a multi-service Cognitive Services resource.\n",
    " \n",
    " After creating your Azure resource, you will need the following information to use it from a client application through one of the supported SDKs:\n",
    "\n",
    "The location in which the resource is deployed (for example, eastus)\n",
    "One of the keys assigned to your resource.\n",
    "\n",
    "The Speech service supports speech recognition through two REST APIs:\n",
    "\n",
    "    The Speech-to-text API, which is the primary way to perform speech recognition.\n",
    "    The Speech-to-text Short Audio API, which is optimized for short streams of audio (up to 60 seconds).\n",
    "\n",
    "\n",
    "<h3>Using the Speech-to-text SDK</h3>\n",
    "\n",
    "1. Use a SpeechConfig object to encapsulate the information required to connect to your Speech resource. Specifically, its location and key.\n",
    "2. Optionally, use an AudioConfig to define the input source for the audio to be transcribed. By default, this is the default system microphone, but you can also specify an audio file.\n",
    "3. Use the SpeechConfig and AudioConfig to create a SpeechRecognizer object. This object is a proxy client for the Speech-to-text API.\n",
    "4. Use the methods of the SpeechRecognizer object to call the underlying API functions. For example, the RecognizeOnceAsync() method uses the Speech service to asynchronously transcribe a single spoken utterance.\n",
    "4. Process the response from the Speech service. In the case of the RecognizeOnceAsync() method, the result is a \n",
    "5. SpeechRecognitionResult object that includes the following properties:\n",
    "        Duration\n",
    "        OffsetInTicks\n",
    "        Properties\n",
    "        Reason\n",
    "        ResultId\n",
    "        Text\n",
    "        \n",
    "If the operation was successful, the Reason property has the enumerated value RecognizedSpeech, and the Text property contains the transcription. Other possible values for Result include NoMatch (indicating that the audio was successfully parsed but no speech was recognized) or Canceled, indicating that an error occurred (in which case, you can check the Properties collection for the CancellationReason property to determine what went wrong.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Use the text-to-speech API</h3>\n",
    "\n",
    "Similarly to Speech-to-text APIs, the Speech service offers two REST APIs for speech synthesis:\n",
    "\n",
    "    The Text-to-speech API, which is the primary way to perform speech synthesis.\n",
    "    The Text-to-speech Long Audio API, which is designed to support batch operations that convert large volumes of text to audio - for example to generate an audio-book from the source text.\n",
    "    \n",
    "<h3>Using the Text-to-speech SDK</h3>\n",
    "\n",
    "1. Use a SpeechConfig object to encapsulate the information required to connect to your Speech resource. Specifically, its location and key.\n",
    "2. Optionally, use an AudioConfig to define the output device for the speech to be synthesized. By default, this is the default system speaker, but you can also specify an audio file, or by explicitly setting this value to a null value, you can process the audio stream object that is returned directly.\n",
    "3. Use the SpeechConfig and AudioConfig to create a SpeechSynthesizer object. This object is a proxy client for the Text-to-speech API.\n",
    "4. Use the methods of the SpeechSynthesizer object to call the underlying API functions. For example, the SpeakTextAsync() method uses the Speech service to convert text to spoken audio.\n",
    "5. Process the response from the Speech service. In the case of the SpeakTextAsync method, the result is a SpeechSynthesisResult object that contains the following properties:\n",
    "        AudioData\n",
    "        Properties\n",
    "        Reason\n",
    "        ResultId\n",
    "When speech has been successfully synthesized, the Reason property is set to the SynthesizingAudioCompleted enumeration and the AudioData property contains the audio stream (which, depending on the AudioConfig may have been automatically sent to a speaker or file).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Audio format</h3>\n",
    "\n",
    "The Speech service supports multiple output formats for the audio stream that is generated by speech synthesis. Depending on your specific needs, you can choose a format based on the required:\n",
    "\n",
    "    Audio file type\n",
    "    Sample-rate\n",
    "    Bit-depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify the required output format, use the SetSpeechSynthesisOutputFormat method of the SpeechConfig object:\n",
    "\n",
    "speechConfig.SetSpeechSynthesisOutputFormat(SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Voices</h3>\n",
    "\n",
    "The Speech service provides multiple voices that you can use to personalize your speech-enabled applications. There are two kinds of voice that you can use:\n",
    "\n",
    "    Standard voices - synthetic voices created from audio samples.\n",
    "    Neural voices - more natural sounding voices created using deep neural networks.\n",
    "Voices are identified by names that indicate a locale and a person's name - for example en-GB-George.\n",
    "\n",
    "To specify a voice for speech synthesis in the SpeechConfig, set its SpeechSynthesisVoiceName property to the voice you want to use:\n",
    "\n",
    "speechConfig.SpeechSynthesisVoiceName = \"en-GB-George\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Use Speech Synthesis Markup Language</h3>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" \n",
    "                     xmlns:mstts=\"https://www.w3.org/2001/mstts\" xml:lang=\"en-US\"> \n",
    "    <voice name=\"en-US-AriaNeural\"> \n",
    "        <mstts:express-as style=\"cheerful\"> \n",
    "          I say tomato \n",
    "        </mstts:express-as> \n",
    "    </voice> \n",
    "    <voice name=\"en-US-GuyNeural\"> \n",
    "        I say <phoneme alphabet=\"sapi\" ph=\"t ao m ae t ow\"> tomato </phoneme>. \n",
    "        <break strength=\"weak\"/>Lets call the whole thing off! \n",
    "    </voice> \n",
    "</speak>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This SSML specifies a spoken dialog between two different neural voices, like this:\n",
    "\n",
    "    Ariana (cheerfully): \"I say tomato:\n",
    "    Guy: \"I say tomato (pronounced tom-ah-toe) ... Let's call the whole thing off!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit an SSML description to the Speech service, you can use the SpeakSsmlAsync() method, like this:\n",
    "\n",
    "speechSynthesizer.SpeakSsmlAsync(ssml_string);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-49556fba7ce1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Import namespaces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "#speaking-clock.py\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Import namespaces\n",
    "# Import namespaces\n",
    "import azure.cognitiveservices.speech as speech_sdk\n",
    "from playsound import playsound\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        global speech_config\n",
    "\n",
    "        # Get Configuration Settings\n",
    "        load_dotenv()\n",
    "        cog_key = os.getenv('COG_SERVICE_KEY')\n",
    "        cog_region = os.getenv('COG_SERVICE_REGION')\n",
    "\n",
    "        # Configure speech service\n",
    "        # Configure speech service\n",
    "        speech_config = speech_sdk.SpeechConfig(cog_key, cog_region)\n",
    "        print('Ready to use speech service in:', speech_config.region)\n",
    "\n",
    "        # Get spoken input\n",
    "        command = TranscribeCommand()\n",
    "        if command.lower() == 'what time is it?':\n",
    "            TellTime()\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "def TranscribeCommand():\n",
    "    command = ''\n",
    "\n",
    "    # Configure speech recognition\n",
    "# Configure speech recognition\n",
    "    audioFile = 'time.wav'\n",
    "    playsound(audioFile)\n",
    "    audio_config = speech_sdk.AudioConfig(filename=audioFile)\n",
    "    speech_recognizer = speech_sdk.SpeechRecognizer(speech_config, audio_config)\n",
    "\n",
    "    # Process speech input\n",
    "    # Process speech input\n",
    "    speech = speech_recognizer.recognize_once_async().get()\n",
    "    if speech.reason == speech_sdk.ResultReason.RecognizedSpeech:\n",
    "        command = speech.text\n",
    "        print(command)\n",
    "    else:\n",
    "        print(speech.reason)\n",
    "        if speech.reason == speech_sdk.ResultReason.Canceled:\n",
    "            cancellation = speech.cancellation_details\n",
    "            print(cancellation.reason)\n",
    "            print(cancellation.error_details)\n",
    "\n",
    "    # Return the command\n",
    "    return command\n",
    "\n",
    "\n",
    "def TellTime():\n",
    "    now = datetime.now()\n",
    "    response_text = 'The time is {}:{:02d}'.format(now.hour,now.minute)\n",
    "\n",
    "\n",
    "    # Configure speech synthesis\n",
    "    # Configure speech synthesis\n",
    "    speech_config.speech_synthesis_voice_name = \"en-GB-RyanNeural\"\n",
    "    speech_synthesizer = speech_sdk.SpeechSynthesizer(speech_config)\n",
    "\n",
    "    # Synthesize spoken output\n",
    "    # Synthesize spoken output\n",
    "    speak = speech_synthesizer.speak_text_async(response_text).get()\n",
    "    if speak.reason != speech_sdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        print(speak.reason)\n",
    "\n",
    "    # Print the response\n",
    "    print(response_text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Speech Translation</h3>\n",
    "\n",
    "Before you can use the Speech service, you need to create a Speech resource in your Azure subscription. You can use either a dedicated Speech resource or a multi-service Cognitive Services resource.\n",
    "\n",
    "<h3>Translate speech to text</h3>\n",
    "\n",
    "1. Use a SpeechConfig object to encapsulate the information required to connect to your Speech resource. Specifically, its location and key.\n",
    "2. Use a SpeechTranslationConfig object to specify the speech recognition language (the language in which the input speech is spoken) and the target languages into which it should be translated.\n",
    "Optionally, use an AudioConfig to define the input source for the audio to be transcribed. By default, this is the default system microphone, but you can also specify an audio file.\n",
    "3. Use the SpeechConfig, SpeechTranslationConfig, and AudioConfig to create a TranslationRecognizer object. This object is a proxy client for the Speech service translation API.\n",
    "4. Use the methods of the TranslationRecognizer object to call the underlying API functions. For example, the RecognizeOnceAsync() method uses the Speech service to asynchronously translate a single spoken utterance.\n",
    "5. Process the response from the Speech service. In the case of the RecognizeOnceAsync() method, the result is a SpeechRecognitionResult object that includes the following properties:\n",
    "        Duration\n",
    "        OffsetInTicks\n",
    "        Properties\n",
    "        Reason\n",
    "        ResultId\n",
    "        Text\n",
    "        Translations\n",
    "If the operation was successful, the Reason property has the enumerated value RecognizedSpeech, the Text property contains the transcription in the original language, and the Translations property contains a dictionary of the translations (using the two-character ISO language code, such as \"en\" for English, as a key)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Synthesize translations</h3>\n",
    "\n",
    "The TranslationRecognizer returns translated transcriptions of spoken input - essentially translating audible speech to text.\n",
    "\n",
    "You can also synthesize the translation as speech to create speech-to-speech translation solutions. There are two ways you can accomplish this.\n",
    "\n",
    "Event-based synthesis\n",
    "\n",
    "When you want to perform 1:1 translation (translating from one source language into a single target language), you can use event-based synthesis to capture the translation as an audio stream. To do this, you need to:\n",
    "\n",
    "1. Specify the desired voice for the translated speech in the TranslationConfig.\n",
    "2. Create an event handler for the TranslationRecognizer object's Synthesizing event.\n",
    "3. In the event handler, use the GetAudio() method of the Result parameter to retrieve the byte stream of translated audio.\n",
    "\n",
    "Manual synthesis\n",
    "\n",
    "Manual synthesis is an alternative approach to event-based synthesis that doesn't require you to implement an event handler. You can use manual synthesis to generate audio translations for one or more target languages.\n",
    "\n",
    "Manual synthesis of translations is essentially just the combination of two separate operations in which you:\n",
    "\n",
    "1. Use a TranslationRecognizer to translate spoken input into text transcriptions in one or more target languages.\n",
    "2. Iterate through the Translations dictionary in the result of the translation operation, using a SpeechSynthesizer to synthesize an audio stream for each language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install azure-cognitiveservices-speech==1.19.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install playsound==1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-011d4410a382>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "#translator.py\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Import namespaces\n",
    "# Import namespaces\n",
    "import azure.cognitiveservices.speech as speech_sdk\n",
    "from playsound import playsound\n",
    "def main():\n",
    "    try:\n",
    "        global speech_config\n",
    "        global translation_config\n",
    "\n",
    "        # Get Configuration Settings\n",
    "        load_dotenv()\n",
    "        cog_key = os.getenv('COG_SERVICE_KEY')\n",
    "        cog_region = os.getenv('COG_SERVICE_REGION')\n",
    "\n",
    "        # Configure translation\n",
    "        # Configure translation\n",
    "        translation_config = speech_sdk.translation.SpeechTranslationConfig(cog_key, cog_region)\n",
    "        translation_config.speech_recognition_language = 'en-US'\n",
    "        translation_config.add_target_language('fr')\n",
    "        translation_config.add_target_language('es')\n",
    "        translation_config.add_target_language('hi')\n",
    "        print('Ready to translate from',translation_config.speech_recognition_language)\n",
    "\n",
    "        # Configure speech\n",
    "        # Configure speech\n",
    "        speech_config = speech_sdk.SpeechConfig(cog_key, cog_region)\n",
    "\n",
    "        # Get user input\n",
    "        targetLanguage = ''\n",
    "        while targetLanguage != 'quit':\n",
    "            targetLanguage = input('\\nEnter a target language\\n fr = French\\n es = Spanish\\n hi = Hindi\\n Enter anything else to stop\\n').lower()\n",
    "            if targetLanguage in translation_config.target_languages:\n",
    "                Translate(targetLanguage)\n",
    "            else:\n",
    "                targetLanguage = 'quit'\n",
    "                \n",
    "\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "def Translate(targetLanguage):\n",
    "    translation = ''\n",
    "\n",
    "    # Translate speech\n",
    "    # Translate speech\n",
    "    # audio_config = speech_sdk.AudioConfig(use_default_microphone=True)\n",
    "    # translator = speech_sdk.translation.TranslationRecognizer(translation_config, audio_config)\n",
    "    # print(\"Speak now...\")\n",
    "    # result = translator.recognize_once_async().get()\n",
    "    # print('Translating \"{}\"'.format(result.text))\n",
    "    # translation = result.translations[targetLanguage]\n",
    "    # print(translation)\n",
    "\n",
    "    # Translate speech\n",
    "    audioFile = 'station.wav'\n",
    "    playsound(audioFile)\n",
    "    audio_config = speech_sdk.AudioConfig(filename=audioFile)\n",
    "    translator = speech_sdk.translation.TranslationRecognizer(translation_config, audio_config)\n",
    "    print(\"Getting speech from file...\")\n",
    "    result = translator.recognize_once_async().get()\n",
    "    print('Translating \"{}\"'.format(result.text))\n",
    "    translation = result.translations[targetLanguage]\n",
    "    print(translation)\n",
    "\n",
    "    # Synthesize translation\n",
    "    # Synthesize translation\n",
    "    voices = {\n",
    "            \"fr\": \"fr-FR-HenriNeural\",\n",
    "            \"es\": \"es-ES-ElviraNeural\",\n",
    "            \"hi\": \"hi-IN-MadhurNeural\"\n",
    "    }\n",
    "    speech_config.speech_synthesis_voice_name = voices.get(targetLanguage)\n",
    "    speech_synthesizer = speech_sdk.SpeechSynthesizer(speech_config)\n",
    "    speak = speech_synthesizer.speak_text_async(translation).get()\n",
    "    if speak.reason != speech_sdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        print(speak.reason)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create a Language Understanding app</h3>\n",
    "\n",
    "    Provision Azure resources for the Language Understanding service.\n",
    "    Define intents and utterances.\n",
    "    Define entities.\n",
    "    Use patterns to differentiate similar utterances.\n",
    "    Use pre-built models.\n",
    "    Train, test, publish, and review a model.\n",
    "    \n",
    "To use the Language Understanding service to develop a natural language understanding solution, you require two kinds of resource in your Azure subscription:\n",
    "\n",
    "1. An Authoring resource (which must be a Language Understanding - Authoring resource) that you can use to train your language understanding model.\n",
    "2. A Prediction resource (which can be a Language Understanding - Prediction or Cognitive Services resource) to host your trained model and process requests from client applications.\n",
    "\n",
    "The Prediction resource prioritizes the client requests by giving you more prediction endpoint requests per month than the Authoring resource for the purposes of supporting client applications.\n",
    "\n",
    "Utterances are the phrases that a user might enter when interacting with an application that uses your Language Understanding model. An intent represents a task or action the user wants to perform, or more simply the meaning of an utterance. You create a model by defining intents and associating them with one or more utterances.\n",
    "\n",
    "In addition to the intents that you define, every model includes None intent that you should use to explicitly identify utterances that a user might submit, but for which there is no specific action required (for example, conversational greetings like \"hello\") or that fall outside of the scope of the domain for this model.\n",
    "\n",
    "Entities are used to add specific context to intents. For example, you might define a TurnOnDevice intent that can be applied to multiple devices, and use entities to define the different devices.\n",
    "\n",
    "You can define entities in a number of ways:\n",
    "\n",
    "    1. Machine learned entities are the most flexible kind of entity, and should be used in most cases. You define a machine learned entity with a suitable name, and then associate words or phrases with it in training utterances. When you train your model, it learns to match the appropriate elements in the utterances with the entity.\n",
    "    2. List entities are useful when you need an entity with a specific set of possible values - for example, days of the week. You can include synonyms in a list entity definition, so you could define a DayOfWeek entity that includes the values \"Sunday\", \"Monday\", \"Tuesday\", and so on; each with synonyms like \"Sun\", \"Mon\", \"Tue\", and so on.\n",
    "    3. Regular Expression or RegEx entities are useful when an entity can be identified by matching a particular format of string. For example, a date in the format MM/DD/YYYY, or a flight number in the format AB-1234.\n",
    "    4. Pattern.any() entities are used with patterns, which are discussed in the next topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Use patterns to differentiate similar utterances</h3>\n",
    "\n",
    "You could associate utterances for every possible entity with all three intents. However, a more efficient way to train the model is to define patterns that include utterance templates, like this:\n",
    "\n",
    "TurnOnDevice:\n",
    "    1. \"Turn the {DeviceName} on.\"\n",
    "    2. \"Switch the {DeviceName} on.\"\n",
    "    3. \"Turn on the {DeviceName}.\"\n",
    "GetDeviceStatus:\n",
    "    1. \"Is the {DeviceName} on[?]\"\n",
    "TurnOffDevice:\n",
    "    1. \"Turn the {DeviceName} off.\"\n",
    "    2. \"Switch the {DeviceName} off.\"\n",
    "    3. \"Turn off the {DeviceName}.\"\n",
    "These utterances include a placeholder for a Pattern.any() entity named DeviceName. reducing the number of utterances required to train the model. Patterns can make use of optional elements, such as punctuation to provide additional cues that help identify the appropriate intent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authouring resourse has to be a Language Understanding- Authouring<br>\n",
    "However, prediction resource can be a LU or cognitive service resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Set publishing configuration options</h3>\n",
    "\n",
    "Every Language Understanding app has two publishing slots:\n",
    "\n",
    "    Staging. Use this slot to publish and test new versions of your language model without disrupting production applications.\n",
    "    Production. Use this slot for \"live\" models that are used by production applications.\n",
    "\n",
    "<h3>Publish settings</h3>\n",
    "\n",
    "Regardless of which slot you publish your Language Understanding app to, you can configure the following publish settings to enable specific behavior:\n",
    "\n",
    "    Sentiment analysis. Enable this to include a sentiment score from 0 (negative) to 1 (positive) in predictions. This score reflects the sentiment of the input utterance.\n",
    "    Spelling correction. Enable this to use the Bing Spell Check service to correct the spelling on input utterances before intent prediction.\n",
    "    Speech priming. Enable this if you plan to use the language model with the Speech service. This option sends the model to the Speech service ahead of prediction to improve intent recognition from spoken input.\n",
    "    \n",
    "To consume your Language Understanding model in a client application, you can use the REST APIs or one of the programming language-specific SDKs.\n",
    "\n",
    "Regardless of the approach used, requests for predictions are sent to a published slot (production or staging) and include the following parameters:\n",
    "\n",
    "    query - the utterance text to be analyzed.\n",
    "    show-all-intents - indicates whether to include all identified intents and their scores, or only the most likely intent.\n",
    "    verbose - used to include additional metadata in the results, such as the start index and length of strings identified as entities,\n",
    "    log used to record queries and results for use in Active Learning.\n",
    "\n",
    "<h3>Prediction results</h3>\n",
    "\n",
    "A typical response might look similar to this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-2d6c2dae1deb>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-2d6c2dae1deb>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    },\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "{\n",
    "  \"query\": \"What's the time in Edinburgh?\",\n",
    "  \"prediction\": {\n",
    "    \"topIntent\": \"GetTime\",\n",
    "    \"intents\": {\n",
    "      \"GetTime\": {\n",
    "        \"score\": 0.9978\n",
    "      },\n",
    "      ...\n",
    "    },\n",
    "    \"entities\": {\n",
    "      \"location\": [\"Edinburgh\"],\n",
    "      ...\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Use a container</h3>\n",
    "\n",
    "Like many other cognitive services, the Language Understanding service can also be deployed as a container, running in a local Docker host, an Azure Container Instance (ACI), or in an Azure Kubernetes Service (AKS) cluster.\n",
    "\n",
    "1. Download the container image\n",
    "2. Export the model for a container\n",
    "3. Run the container with required parameters:\n",
    "        Prediction endpoint for billing\n",
    "        Prediction key\n",
    "        EULA acceptance\n",
    "        Mount points (input for exported model, output for logs)\n",
    "4. Use the container to predict intents for client apps\n",
    "5. The container sends usage metrics to your prediction resource for billing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps: pull docker image -> export Language understanding app in .gz format -> run docker container with image and mount input folder with the exported model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Understand Language Understanding and Speech service integration</h3>\n",
    "\n",
    "To use the Speech SDK with a Language Understanding model, enable the Speech priming publishing setting for your Language Understanding endpoint, and use the Speech SDK to write code that uses your Language Understanding prediction resource\n",
    "\n",
    "To use a Language Understanding model from the Speech SDK, your code should follow this pattern:\n",
    "\n",
    "1. Use a SpeechConfig object to encapsulate the information required to connect to your Language Understanding prediction resource (not a Speech resource). Specifically, the SpeechConfig must be configured with the location and key of the Language Understanding prediction resource.\n",
    "2. Optionally, use an AudioConfig to define the input source for the speech to be analyzed. By default, this is the default system microphone, but you can also specify an audio file.\n",
    "3. Use the SpeechConfig and AudioConfig to create an IntentRecognizer object, and add the model and the intents you want to recognize to its configuration.\n",
    "4. Use the methods of the IntentRecognizer object to submit utterances to the Language understanding prediction endpoint. For example, the RecognizeOnceAsync() method submits a single spoken utterance.\n",
    "5. Process the response. In the case of the RecognizeOnceAsync() method, the result is an IntentRecognitionResult object that includes the following properties:\n",
    "\n",
    "        Duration\n",
    "        IntendId\n",
    "        OffsetInTicks\n",
    "        Properties\n",
    "        Reason\n",
    "        ResultId\n",
    "        Text\n",
    "\n",
    "If the operation was successful, the Reason property has the enumerated value RecognizedIntent, and the IntentId property contains the top intent name. Full details of the Language Understanding prediction can be found in the Properties property, which includes the full JSON prediction.\n",
    "\n",
    "Other possible values for Result include RecognizedSpeech, which indicates that the speech was successfully transcribed (the transcription is in the Text property), but no matching intent was identified. If the result is NoMatch, the audio was successfully parsed but no speech was recognized, and if the result is Canceled, an error occurred (in which case, you can check the Properties collection for the CancellationReason property to determine what went wrong.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable Speech Priming while publishing your application on luis.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-625a55620794>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#speaking-clock-client.py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "#speaking-clock-client.py\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta, date\n",
    "from dateutil.parser import parse as is_date\n",
    "import playsound\n",
    "# Import namespaces\n",
    "# Import namespaces\n",
    "import azure.cognitiveservices.speech as speech_sdk\n",
    "\n",
    "def main():\n",
    "\n",
    "    try:\n",
    "        # Get Configuration Settings\n",
    "        load_dotenv()\n",
    "        lu_app_id = os.getenv('LU_APP_ID')\n",
    "        lu_prediction_region = os.getenv('LU_PREDICTION_REGION')\n",
    "        lu_prediction_key = os.getenv('LU_PREDICTION_KEY')\n",
    "\n",
    "        # Configure speech service and get intent recognizer\n",
    "        # Configure speech service and get intent recognizer\n",
    "        #speech_config = speech_sdk.SpeechConfig(subscription=lu_prediction_key, region=lu_prediction_region)\n",
    "        #audio_config = speech_sdk.AudioConfig(use_default_microphone=True)\n",
    "        #recognizer = speech_sdk.intent.IntentRecognizer(speech_config, audio_config)\n",
    "\n",
    "        # Configure speech service and get intent recognizer\n",
    "        audioFile = 'time-in-london.wav'\n",
    "        playsound(audioFile)\n",
    "        speech_config = speech_sdk.SpeechConfig(subscription=lu_prediction_key, region=lu_prediction_region)\n",
    "        audio_config = speech_sdk.AudioConfig(filename=audioFile)\n",
    "        recognizer = speech_sdk.intent.IntentRecognizer(speech_config, audio_config)\n",
    "\n",
    "        # Get the model from the AppID and add the intents we want to use\n",
    "        # Get the model from the AppID and add the intents we want to use\n",
    "        model = speech_sdk.intent.LanguageUnderstandingModel(app_id=lu_app_id)\n",
    "        intents = [\n",
    "            (model, \"GetTime\"),\n",
    "            (model, \"GetDate\"),\n",
    "            (model, \"GetDay\"),\n",
    "            (model, \"None\")\n",
    "        ]\n",
    "        recognizer.add_intents(intents)\n",
    "\n",
    "        # Process speech input\n",
    "        # Process speech input\n",
    "        intent = ''\n",
    "        result = recognizer.recognize_once_async().get()\n",
    "        if result.reason == speech_sdk.ResultReason.RecognizedIntent:\n",
    "            intent = result.intent_id\n",
    "            print(\"Query: {}\".format(result.text))\n",
    "            print(\"Intent: {}\".format(intent))\n",
    "            json_response = json.loads(result.intent_json)\n",
    "            print(\"JSON Response:\\n{}\\n\".format(json.dumps(json_response, indent=2)))\n",
    "\n",
    "            # Get the first entity (if any)\n",
    "            # Get the first entity (if any)\n",
    "            entity_type = ''\n",
    "            entity_value = ''\n",
    "            if len(json_response[\"entities\"]) > 0:\n",
    "                entity_type = json_response[\"entities\"][0][\"type\"]\n",
    "                entity_value = json_response[\"entities\"][0][\"entity\"]\n",
    "                print(entity_type + ': ' + entity_value)\n",
    "            # Apply the appropriate action\n",
    "            # Apply the appropriate action\n",
    "            if intent == 'GetTime':\n",
    "                location = 'local'\n",
    "                # Check for entities\n",
    "                if entity_type == 'Location':\n",
    "                    location = entity_value\n",
    "                # Get the time for the specified location\n",
    "                print(GetTime(location))\n",
    "\n",
    "            elif intent == 'GetDay':\n",
    "                date_string = date.today().strftime(\"%m/%d/%Y\")\n",
    "                # Check for entities\n",
    "                if entity_type == 'Date':\n",
    "                    date_string = entity_value\n",
    "                # Get the day for the specified date\n",
    "                print(GetDay(date_string))\n",
    "\n",
    "            elif intent == 'GetDate':\n",
    "                day = 'today'\n",
    "                # Check for entities\n",
    "                if entity_type == 'Weekday':\n",
    "                    # List entities are lists\n",
    "                    day = entity_value\n",
    "                # Get the date for the specified day\n",
    "                print(GetDate(day))\n",
    "\n",
    "            else:\n",
    "                # Some other intent (for example, \"None\") was predicted\n",
    "                print('You said {}'.format(result.text))\n",
    "                if result.text.lower().replace('.', '') == 'stop':\n",
    "                    intent = result.text\n",
    "                else:\n",
    "                    print('Try asking me for the time, the day, or the date.')\n",
    "\n",
    "\n",
    "        elif result.reason == speech_sdk.ResultReason.RecognizedSpeech:\n",
    "            # Speech was recognized, but no intent was identified.\n",
    "            intent = result.text\n",
    "            print(\"I don't know what {} means.\".format(intent))\n",
    "        elif result.reason == speech_sdk.ResultReason.NoMatch:\n",
    "            # Speech wasn't recognized\n",
    "            print(\"Sorry. I didn't understand that.\")\n",
    "        elif result.reason == speech_sdk.ResultReason.Canceled:\n",
    "            # Something went wrong\n",
    "            print(\"Intent recognition canceled: {}\".format(result.cancellation_details.reason))\n",
    "            if result.cancellation_details.reason == speech_sdk.CancellationReason.Error:\n",
    "                print(\"Error details: {}\".format(result.cancellation_details.error_details))\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "\n",
    "def GetTime(location):\n",
    "    time_string = ''\n",
    "\n",
    "    # Note: To keep things simple, we'll ignore daylight savings time and support only a few cities.\n",
    "    # In a real app, you'd likely use a web service API (or write  more complex code!)\n",
    "    # Hopefully this simplified example is enough to get the the idea that you\n",
    "    # use LU to determine the intent and entitites, then implement the appropriate logic\n",
    "\n",
    "    if location.lower() == 'local':\n",
    "        now = datetime.now()\n",
    "        time_string = '{}:{:02d}'.format(now.hour,now.minute)\n",
    "    elif location.lower() == 'london':\n",
    "        utc = datetime.utcnow()\n",
    "        time_string = '{}:{:02d}'.format(utc.hour,utc.minute)\n",
    "    elif location.lower() == 'sydney':\n",
    "        time = datetime.utcnow() + timedelta(hours=11)\n",
    "        time_string = '{}:{:02d}'.format(time.hour,time.minute)\n",
    "    elif location.lower() == 'new york':\n",
    "        time = datetime.utcnow() + timedelta(hours=-5)\n",
    "        time_string = '{}:{:02d}'.format(time.hour,time.minute)\n",
    "    elif location.lower() == 'nairobi':\n",
    "        time = datetime.utcnow() + timedelta(hours=3)\n",
    "        time_string = '{}:{:02d}'.format(time.hour,time.minute)\n",
    "    elif location.lower() == 'tokyo':\n",
    "        time = datetime.utcnow() + timedelta(hours=9)\n",
    "        time_string = '{}:{:02d}'.format(time.hour,time.minute)\n",
    "    elif location.lower() == 'delhi':\n",
    "        time = datetime.utcnow() + timedelta(hours=5.5)\n",
    "        time_string = '{}:{:02d}'.format(time.hour,time.minute)\n",
    "    else:\n",
    "        time_string = \"I don't know what time it is in {}\".format(location)\n",
    "    \n",
    "    return time_string\n",
    "\n",
    "def GetDate(day):\n",
    "    date_string = 'I can only determine dates for today or named days of the week.'\n",
    "\n",
    "    weekdays = {\n",
    "        \"monday\":0,\n",
    "        \"tuesday\":1,\n",
    "        \"wednesday\":2,\n",
    "        \"thusday\":3,\n",
    "        \"friday\":4,\n",
    "        \"saturday\":5,\n",
    "        \"sunday\":6\n",
    "    }\n",
    "\n",
    "    today = date.today()\n",
    "\n",
    "    # To keep things simple, assume the named day is in the current week (Sunday to Saturday)\n",
    "    day = day.lower()\n",
    "    if day == 'today':\n",
    "        date_string = today.strftime(\"%m/%d/%Y\")\n",
    "    elif day in weekdays:\n",
    "        todayNum = today.weekday()\n",
    "        weekDayNum = weekdays[day]\n",
    "        offset = weekDayNum - todayNum\n",
    "        date_string = (today + timedelta(days=offset)).strftime(\"%m/%d/%Y\")\n",
    "\n",
    "    return date_string\n",
    "\n",
    "def GetDay(date_string):\n",
    "    # Note: To keep things simple, dates must be entered in US format (MM/DD/YYYY)\n",
    "    try:\n",
    "        date_object = datetime.strptime(date_string, \"%m/%d/%Y\")\n",
    "        day_string = date_object.strftime(\"%A\")\n",
    "    except:\n",
    "        day_string = 'Enter a date in MM/DD/YYYY format.'\n",
    "    return day_string\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create QnA application</h3>\n",
    "\n",
    "To create a knowledge base:\n",
    "\n",
    "1. Create a Language resource in your Azure subscription.\n",
    "    Enable the question answering feature.\n",
    "    Create or select an Azure Cognitive Search resource to host the knowledge base index.\n",
    "2. In Language Studio, select the Language resource and create a Custom question answering project.\n",
    "3. Name the knowledge base.\n",
    "4. Add one or more data sources to populate the knowledge base:\n",
    "    URLs for web pages containing FAQs.\n",
    "    Files containing structured text from which questions and answers can be derived.\n",
    "    Pre-defined chit-chat datasets that include common conversational questions and responses in a specified style.\n",
    "5. Create the knowledge base and edit question and answer pairs in the portal.\n",
    "\n",
    "You can enable multi-turn responses when importing questions and answers from an existing web page or document based on its structure, or you can explicitly define follow-up prompts and responses for existing question and answer pairs.\n",
    "\n",
    "For example, suppose an initial question for a travel booking knowledge base is \"How can I cancel a reservation?\". A reservation might refer to a hotel or a flight, so a follow-up prompt is required to clarify this detail. The answer might consist of text such as \"Cancellation policies depend on the type of reservation\" and include follow-up prompts with links to answers about canceling flights and canceling hotels.\n",
    "\n",
    "You can test your knowledge base interactively in Language Studio, submitting questions and reviewing the answers that are returned. \n",
    "\n",
    "When you are happy with the performance of your knowledge base, you can deploy it to a REST endpoint that client applications can use to submit questions and receive answers.\n",
    "\n",
    "To consume the published knowledge base, you can use the REST interface.\n",
    "\n",
    "The minimal request body for the function contains a question, like this:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What do I need to do to cancel a reservation?'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"question\": \"What do I need to do to cancel a reservation?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response includes the closest question match that was found in the knowledge base, along with the associated answer, the confidence score, and other metadata about the question and answer pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'false' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-cb7380071c37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m       \u001b[1;34m\"metadata\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m       \"dialog\": {\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[1;34m\"isContextOnly\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;34m\"prompts\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m       }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'false' is not defined"
     ]
    }
   ],
   "source": [
    "{\n",
    "  \"answers\": [\n",
    "    {\n",
    "      \"questions\": [\n",
    "        \"How can I cancel a reservation?\"\n",
    "      ],\n",
    "      \"answer\": \"Call us on 555 123 4567 to cancel a reservation.\",\n",
    "      \"confidenceScore\": 1.0,\n",
    "      \"id\": 6,\n",
    "      \"source\": \"https://margies-travel.com/faq\",\n",
    "      \"metadata\": {},\n",
    "      \"dialog\": {\n",
    "        \"isContextOnly\": false,\n",
    "        \"prompts\": []\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the ＋Create a resource button, search for Language, and create a Language service resource.\n",
    "\n",
    "Click Select on the Custom question answering block. Then click Continue to create your resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Custom Question Answering uses Azure Search to index and query the knowledge base of questions and answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create a bot using bot framework sdk</h3>\n",
    "\n",
    "Bot solutions on Microsoft Azure are supported by the following technologies:\n",
    "    1. Azure Bot Service. A cloud service that enables bot delivery through one or more channels, and integration with other services.\n",
    "    2. Bot Framework Service. A component of Azure Bot Service that provides a REST API for handling bot activities.\n",
    "    3. Bot Framework SDK. A set of tools and libraries for end-to-end bot development that abstracts the REST interface, enabling bot development in a range of programming languages.\n",
    "\n",
    "The Bot Framework SDK provides an extensive set of tools and libraries that software engineers can use to develop bots.\n",
    "\n",
    "<h3>Bot templates</h3>\n",
    "\n",
    "The easiest way to get started with the Bot Framework SDK is to base your new bot on one the templates it provides:\n",
    "\n",
    "    Empty Bot - a basic bot skeleton.\n",
    "    Echo Bot - a simple \"hello world\" sample in which the bot responds to messages by echoing the message text back to the user.\n",
    "    Core Bot - a more comprehensive bot that includes common bot functionality, such as integration with the Language Understanding service.\n",
    "\n",
    "Bot application classes and logic:\n",
    "    \n",
    "The template bots are based on the Bot class defined in the Bot Framework SDK, which is used to implement the logic in your bot that receives and interprets user input, and responds appropriately. Additionally, bots make use of an Adapter class that handles communication with the user's channel.\n",
    "\n",
    "Conversations in a bot are composed of activities, which represent events such as a user joining a conversation or a message being received. These activities occur within the context of a turn, a two-way exchange between the user and bot. The Bot Framework Service notifies your bot's adapter when an activity occurs in a channel by calling its Process Activity method, and the adapter creates a context for the turn and calls the bot's Turn Handler method to invoke the appropriate logic for the activity.\n",
    "\n",
    "The Bot Framework Emulator is an application that enables you to run your bot a local or remote web application and connect to it from an interactive web chat interface that you can use to test your bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Implement activity handlers and dialogs</h3>\n",
    "\n",
    "The logic for processing the activity can be implemented in multiple ways. The Bot Framework SDK provides classes that can help you build bots that manage conversations using:\n",
    "\n",
    "    Activity handlers: Event methods that you can override to handle different kinds of activities.\n",
    "    Dialogs: More complex patterns for handling stateful, multi-turn conversations.\n",
    "\n",
    "Activity handlers\n",
    "\n",
    "For simple bots with short, stateless interactions, you can use Activity Handlers to implement an event-driven conversation model in which the events are triggered by activities such as users joining the conversation or a message being received. When an activity occurs in a channel, the Bot Framework Service calls the bot adapter's Process Activity function, passing the activity details. The adapter creates a turn context for the activity and passes it to the bot's turn handler, which calls the individual, event-specific activity handler.\n",
    "\n",
    "The ActivityHandler base class includes event methods for the many kinds of common activity, including:\n",
    "\n",
    "    Message received\n",
    "    Members joined the conversation\n",
    "    Members left the conversation\n",
    "    Message reaction received\n",
    "    Bot installed\n",
    "    Others...\n",
    "\n",
    "Turn context\n",
    "\n",
    "An activity occurs within the context of a turn, which represents a single two-way exchange between the user and the bot. Activity handler methods include a parameter for the turn context, which you can use to access relevant information. For example, the activity handler for a message received activity includes the text of the message.\n",
    "\n",
    "Dialogs\n",
    "\n",
    "For more complex conversational flows where you need to store state between turns to enable a multi-turn conversation, you can implement dialogs. The Bot Framework SDK dialogs library provides multiple dialog classes that you can combine to implement the required conversational flow for your bot.\n",
    "\n",
    "There are two common patterns for using dialogs to compose a bot conversation:\n",
    "    1. Component dialogs: A component dialog is a dialog that can contain other dialogs, defined in its dialog set. Often, the initial dialog in the component dialog is a waterfall dialog\n",
    "    2. Adaptive dialogs: An adaptive dialog is another kind of container dialog in which the flow is more flexible, allowing for interruptions, cancellations, and context switches at any point in the conversation. In this style of conversation, the bot initiates a root dialog, which contains a flow of actions (which can include branches and loops), and triggers that can be initiated by actions or by a recognize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
